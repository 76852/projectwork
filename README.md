## 项目简介

本项目旨在评估不同规模LLM对编程题目的记忆能力。通过分析模型在给定题目前缀的情况下生成正确后缀的能力，我们可以了解模型的训练数据覆盖面以及生成一致性。

## 项目结构

```
├── VibeThinker/           # VibeThinker-1.5B模型测试相关文件
│   ├── run_vibethinker.sh    # 运行脚本
│   ├── vibethinker_test.py   # 测试代码
│   └── [日志和结果文件]      # 测试过程生成的日志和结果
├── mistral/               # Mistral-7B模型测试相关文件
│   ├── run3.sh               # 运行脚本
│   ├── mistral_test.py       # 测试代码
│   └── [日志和结果文件]      # 测试过程生成的日志和结果
├── qwen/                  # Qwen2-8B模型测试相关文件
│   ├── run_qwen.sh           # 运行脚本
│   ├── qwen_test.py          # 测试代码
│   └── [日志和结果文件]      # 测试过程生成的日志和结果
├── dataset/               # 数据集目录
│   └── leetcode_dataset.csv  # 原始LeetCode数据集
├── processed_leetcode_data.json  # 处理后的测试数据
├── ana.md                 # 数据分析报告
├── analysis.md            # 详细分析结果
└── generate_chart.py      # 图表生成脚本
```

## 核心功能

### 1. 模型记忆能力测试

- **前缀-后缀生成测试**：给定题目描述的前半部分（前缀），要求模型生成后半部分（后缀）
- **相似度评估**：使用精确匹配数，平均相似度等指标评估生成内容与真实内容的匹配程度
- **难度级别分析**：按Easy、Medium、Hard三个难度级别分析模型表现
- **精确匹配统计**：统计模型完全正确生成后缀的比例

### 2. 数据处理

- 从LeetCode数据集提取编程题目
- 将题目描述分割为前缀和后缀
- 计算前缀和后缀的长度统计信息

### 3. 结果分析

- 生成相似度分布图表
- 分析不同模型在各种难度下的表现差异
- 评估模型在不同长度区间的生成能力

## 测试的模型

- **Mistral-7B-Instruct-v0.2**：大型7B参数模型
- **Qwen2-8B-Instruct**：大型8B参数模型  
- **VibeThinker-1.5B**：小型1.5B参数模型（用于对比）

## 测试数据集

项目使用了1825道LeetCode编程题目作为测试集，按难度分布如下：
- Easy: 477题
- Medium: 963题
- Hard: 385题

每个题目包含：
- 题目ID、标题、难度级别
- 完整描述、前缀、真实后缀
- 前缀长度、后缀长度统计

## 主要发现

### 1. 模型规模影响
- 大型模型（Mistral-7B、Qwen2-8B）表现远优于小型模型（VibeThinker-1.5B）
- 大型模型平均相似度超过0.4，而小型模型仅为0.047

### 2. 前缀-后缀长度分布
- 前缀平均长度：384.32字符
- 后缀平均长度：417.65字符
- 前缀主要集中在201-500字符区间（占57.6%）
- 后缀主要集中在301-800字符区间（占59.3%）

### 3. 记忆能力限制
- 仅13.1%的后缀在50 tokens限制下可完全生成
- 64.4%的后缀超过300字符，测试了模型的长文本生成能力

## 使用方法

### 环境要求

- Python 3.12
- PyTorch
- Transformers
- NumPy
- Matplotlib（用于图表生成）

### 运行测试

1. 进入对应模型的目录
2. 确保已正确配置运行脚本中的路径
3. 执行运行脚本：

```bash
# 示例：运行Mistral模型测试
cd mistral
./run3.sh
```

## 测试流程

1. **模型加载**：加载预训练的语言模型和分词器
2. **数据加载**：读取处理后的LeetCode测试数据
3. **记忆测试**：对每个题目执行前缀-后缀生成测试
4. **结果评估**：计算相似度和精确匹配率
5. **统计分析**：按难度级别和长度区间进行统计
6. **结果保存**：保存详细测试结果到JSON文件
7. **日志记录**：记录测试过程和关键指标

## 结果评估指标

- **精确匹配率**：完全正确生成的样本比例
- **平均相似度**：生成内容与真实内容的平均余弦相似度
- **成功率**：成功生成结果的样本比例
- **相似度分布**：不同相似度区间的样本分布情况
- **难度分析**：不同难度级别下的表现差异
