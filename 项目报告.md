# 项目实践——面向机器学习的数据清洗方法

## 一、报告内容

### 1. 项目背景与意义
在当今的机器学习领域，数据质量对模型性能和可靠性具有至关重要的影响。然而，现实世界中的数据集常存在各类质量问题，其中标签噪声是最常见且最具挑战性的问题之一。标签噪声指数据集中的错误标注或误导性标注，可能对模型训练和预测产生负面影响。

研究表明，即使小比例的标签噪声也可能导致模型性能显著下降；极端情况下，甚至会使模型学习到错误模式，产生误导性预测结果。这不仅影响模型准确性，还降低其实际应用中的可靠性。

为解决标签噪声问题，研究人员提出多种基于置信学习的方法。置信学习是专门处理标签噪声的机器学习方法，通过估计噪声标签与真实标签的联合分布，识别并校正错误标注样本。其核心利用模型预测概率定位错误标签，优势在于无需超参数调整、能快速识别错误样本，广泛应用于图像分类、语音识别等场景。

### 2. 项目内容
- 学习机器学习基础知识
- 通过阅读论文学习面向机器学习的清洗方法
- 实现数据主动清洗功能

## 二、项目相关概念与技术

### 1. 数据清洗基础
数据清洗是重要的数据预处理步骤，能保证数据质量。通过一系列技术对原始数据进行整理，确保其符合机器学习算法要求。

#### 1.1 缺失值处理
数据集中的缺失值可能导致模型训练不稳定或预测不准。常见处理方法：
- 缺失比例较低时，直接删除含缺失值的记录
- 填充缺失值

#### 1.2 异常值处理
异常值可能干扰模型训练与预测，导致拟合过度或不足。常见处理方法：
- 基于统计方法（如Z-score、IQR）识别并剔除
- 对异常值进行修正

### 2. 置信学习核心概念
#### 2.1 置信学习方法
置信学习是针对标签噪声的机器学习方法，通过估计噪声标签（\( 	ilde{y} \)）与真实标签（\( y^* \)）的联合分布，识别并校正错误标注样本。核心思想是利用模型预测概率定位错误标签，提升模型准确性与鲁棒性。

#### 2.2 噪声联合分布
噪声联合分布指观测标签与真实标签的联合概率分布，用于量化分析标签噪声的统计特性。

#### 2.3 主动标签清理
主动标签清理是基于模型置信度的标注优化方法，通过模型预测概率或确定性评估，优先标注高价值样本（模型最不确定的样本）。

## 三、项目设计

### 1. 系统模块设计

| 模块名称         | 核心功能                                                                 |
|------------------|--------------------------------------------------------------------------|
| 数据预处理模块   | 对原始数据集进行预处理操作 |
| 标签清洗模块     | 基于置信学习方法识别并清洗噪声标签，提升数据集标签质量 |
| 模型训练模块     | 使用清洗后数据训练监督学习模型 |

## 四、项目关键技术与实现

### 1. 数据预处理技术
#### 1.1 数据清理
数据清理是预处理首要任务，目标是修正或删除不正确/干扰数据。主要步骤：
1. **去重与筛选**：删除重复或者无关的数据
2. **结构修复**：统一数据格式（如日期、数值范围）
3. **缺失值处理**：处理缺失数据
4. **数据验证**：通过一致性检查、范围检查等确保数据准确性

#### 1.2 噪声处理
噪声处理用于减少随机误差与异常值的影响。常用方法：
- **分箱（Binning）**：将连续数据分组到离散区间，降低微小误差影响
- **回归（Regression）**：通过变量关系建模，削弱噪声干扰
- **聚类（Clustering）**：通过数据分组识别并处理噪声点

#### 1.3 数据集成
数据集成整合多源数据，支持两种模式：
- **紧耦合**
- **松耦合**

#### 1.4 数据转换
- 正常化 Normalization
- 概念层次结构生成 Concept hierarchy generation
- 平滑 Smoothing
- 聚合 Aggregation

### 2. 先进数据清洗技术
#### 2.1 HoloClean
HoloClean能够修复结构化数据集中的各类错误，包括冲突和拼写错误的值、异常值以及空条目。它通过统计学习和概率推理来处理生成的模型。工作流程分为错误检测、编译和数据修复。HoloClean专注于结构化数据集，旨在识别和修复观察值与真实值不符的单元格数据（错误单元格）。错误检测就是识别出错误的单元格。数据修复就是推断并修正检测到的错误单元格的真实值。HoloClean整合了异构弱信号，提供了结构化数据正确值的证据，从而有效检测和修复错误。

#### 2.2 ActiveClean
ActiveClean是用于数据清洗和修复的交互式机器学习系统。其核心目标在于通过融合用户反馈与自动化技术，清理脏数据。ActiveClean能够迭代式地清洗脏数据。能在干净样本和先前已清理的数据上反复训练模型。适用于线性回归、SVM等凸损失模型。

#### 2.3 TARS系统
通过预测标签清洁后的模型改进程度，判断标签是否值得检查。工作流程分为三个步骤：识别低置信度标签，估计清洁后模型准确性，对比当前模型决定是否清洗。

### 3. 基于置信学习的清洗方法

#### 3.1  Count：估计噪声标签和真实标签的联合分布
step 1：交叉验证--这一步要对数据集进行交叉验证，然后计算置信度阈值
step 2：计算计数矩阵
step 3：标定计数矩阵--目的是让计数总和与人工标记的样本总数相同
step 4：估计噪声标签和真实标签的联合分布

#### 3.2  Clean：找出并过滤掉错误样本
method 1：过滤预测概率最大但和人工标记不一致的数据
method 2：过滤计数矩阵中非对角单元的样本

#### 3.3  Re-Training：过滤错误样本后，重新训练

## 五、参考文献
- A review: Data pre-processing and data augmentation techniques
- Data collection and quality challenges in deep learning: a data-centric AI perspective
- HoloClean: Holistic Data Repairs with Probabilistic Inference
- a benchmark for joint data cleaning and machine learning
- ActiveClean: interactive data cleaning for statistical modeling
- Active label cleaning for improved dataset quality under resource constraints
- Confident Learning: Estimating Uncertainty in Dataset Label

## 六、总结与感悟

这学期，我每周都会投入一定时间在数据清洗的项目实践中，包括阅读论文、了解和使用基础工具等。这些经历让我深刻认识到，想要掌握真正有意义且实用的知识，并非纸上谈兵那么简单，而是需要亲自探索和实践。而且这些知识和技能在未来工作中极有可能派上用场，但大学里通常不会有老师专门教授，这就需要我们主动去了解并深入学习。

我觉得数据清洗的难点在于数据的多样性，不同类型的数据需要采用不同的清洗方法。此外，数据量可能非常庞大，导致数据清洗的时间成本很高。对于简单的项目，数据清洗可能只需十几行代码就能完成。但是在科研领域则需要更先进、高效的技术，远非十几行代码所能解决。

我也深刻理解了数据清洗的重要性。如果数据未经清洗直接用于模型训练，模型的效果可能会大打折扣。例如，一个模型的准确率原本为90%，但如果数据没有经过清洗，其准确率可能会降至80%。这是因为数据中的噪声、异常值和缺失值等因素会对模型的训练产生负面影响。数据清洗的目的正是为了确保模型在训练过程中能够更好地学习数据的特征，从而提升模型的准确率。



